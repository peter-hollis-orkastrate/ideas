# =============================================================================
# OCR Provenance MCP System - Environment Configuration (Local AI Edition)
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
#
# No external API keys required — all AI runs locally via Marker + Ollama.

# -----------------------------------------------------------------------------
# OLLAMA CONFIGURATION (VLM image analysis)
# -----------------------------------------------------------------------------
# URL of your locally running Ollama instance.
# Start Ollama with: ollama serve
# For Docker: use http://host.docker.internal:11434
OLLAMA_BASE_URL=http://localhost:11434

# --- Model 1: VLM — image analysis / description (replaces Google Gemini) ---
# Used to analyse embedded images extracted from documents.
# Must be a vision-capable model. Pull with: ollama pull llava
# Options:
#   llava           — general-purpose, good quality (~4GB VRAM)
#   llava-llama3    — better reasoning (~5GB VRAM)
#   minicpm-v       — multilingual, compact (~3GB VRAM)
#   moondream2      — fastest, smallest (~2GB VRAM)
OLLAMA_VLM_MODEL=llava

# --- Model 2: OCR-assist — document text-extraction tasks (replaces Datalab AI) ---
# Used for structured text extraction, table parsing, and OCR post-processing.
# A vision model is best for scanned/handwritten docs; a pure-text model works
# for clean digital documents. Defaults to OLLAMA_VLM_MODEL if not set.
# Pull with: ollama pull llava   (or a lighter model if speed matters)
OLLAMA_OCR_MODEL=llava

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
OLLAMA_TEMPERATURE=0.1

# Maximum tokens for Ollama responses
OLLAMA_MAX_OUTPUT_TOKENS=8192

# -----------------------------------------------------------------------------
# LOCAL OCR CONFIGURATION (Marker)
# -----------------------------------------------------------------------------
# Marker runs fully offline using local GPU/CPU — no config needed.
# It auto-detects the best hardware: CUDA > MPS > CPU.
#
# For Office files (DOCX/XLSX/PPTX), install LibreOffice:
#   Ubuntu/Debian: sudo apt install libreoffice
#   macOS:         brew install --cask libreoffice

# -----------------------------------------------------------------------------
# EMBEDDING DEVICE (optional)
# -----------------------------------------------------------------------------
# Embedding device: auto | cuda | cuda:0 | mps (Apple Silicon) | cpu
# "auto" detects the best available: CUDA > MPS > CPU
EMBEDDING_DEVICE=auto

# -----------------------------------------------------------------------------
# PYTORCH / CUDA ENVIRONMENT (optional)
# -----------------------------------------------------------------------------
# These are consumed by PyTorch runtime, not the application code directly.
# Uncomment to override PyTorch defaults.
#
# CUDA_VISIBLE_DEVICES=0
# PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
